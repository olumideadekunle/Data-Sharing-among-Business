{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olumideadekunle/Data-Sharing-among-Business/blob/main/Proactive_Insights_for_Employee_Well_being_Predicting_Burnout_with_Machine_Learning_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d300dc7-6636-4a37-97b5-27f4ed1a0a06",
      "metadata": {
        "id": "6d300dc7-6636-4a37-97b5-27f4ed1a0a06"
      },
      "source": [
        "# Proactive Insights for Employee Well-being: Predicting Burnout with Machine Learning\n",
        "\n",
        "In this rapidly paced world, we all seem to have a lot to keep up with and even more to do with more multitasking, task-switching and decline in mental health. In this light, World Mental Health Day was established and is celebrated annually on October 10th to raise awareness about mental health issues and mobilize support worldwide.\n",
        "\n",
        "Having moved around a bit in the data space in the past months, you work as a data scientist at NeuroWell Analytics. Last week Friday in the usual weekly stand-ups the Human Resources Manager raised the issue of **employee burnout** as one of the top-most challenges highlighted often by companies NeuroWell Analytics clientele. Upon this discovery, your boss turns towards you and assigns to you the task of addressing this important issue. He decided it would be best to build a solution predicting employee burnout rates using historical data to proactively address mental health concerns.\n",
        "\n",
        "\n",
        "## Overview - a bit about the company\n",
        "\n",
        "NeuroWell Analytics is a global leader in workplace well-being and productivity solutions. Since 2015, the organization has specialized in leveraging data science and machine learning to enhance employee engagement and mental health. With a team of psychologists, data scientists, and HR specialists, NeuroWell Analytics partners with companies across various industries to create a more resilient and thriving workforce.\n",
        "\n",
        "![NeuroWell](https://drive.google.com/uc?export=view&id=1LbtLJQjK-UKdbqHaUF4lNbUmqFsjdSWH)\n",
        "\n",
        "\n",
        "The organization‚Äôs mission is to empower companies to take proactive steps in addressing mental health challenges by providing actionable insights derived from data. NeuroWell Analytics combines cutting-edge technology with evidence-based research to deliver comprehensive solutions, including predictive analytics, well-being assessments, and customized intervention strategies.\n",
        "\n",
        "Employee burnout is a growing concern globally, impacting productivity, morale, and overall organizational health. Using data provided by your company, you will analyze employee profiles and develop a machine learning model to predict burnout rates based on various factors such as work environment, resource allocation, and mental fatigue scores. Also, you will develop actionable insights to help your company mitigate burnout and foster a healthier work environment.\n",
        "\n",
        "## Objective\n",
        "\n",
        "The primary objectives of this project are:\n",
        "\n",
        "- Data Understanding and Exploration: Analyze the dataset to identify patterns, trends, and correlations related to employee burnout.\n",
        "- Data Preprocessing: Handle missing values, encode categorical data, and normalize numerical features.\n",
        "- Feature Engineering: Derive meaningful features that improve the performance of your machine learning model.\n",
        "- Model Development: Build and evaluate predictive models to estimate employee burnout rates.\n",
        "- Insights and Recommendations: Provide actionable recommendations to reduce burnout based on model outcomes.\n",
        "\n",
        "## About the data\n",
        "\n",
        "| Column Name           | Description                                             |\n",
        "|-----------------------|---------------------------------------------------------|\n",
        "| Employee ID           | Unique ID of the employee                               |\n",
        "| Date of Joining       | Date on which the employee joined the company           |\n",
        "| Gender                | Gender of the employee                                  |\n",
        "| Company Type          | Type of company e.g., Service-based, Product-based      |\n",
        "| WFH Setup Available   | Whether proper work-from-home setup is available or not |\n",
        "| Designation           | Seniority level of the employee in codes               |\n",
        "| Resource Allocation   | Hours allocated per day                                 |\n",
        "| Mental Fatigue Score  | Stress rating provided by employees                     |\n",
        "| Burn Rate             | Rate of saturation or burnout rate [Target]            |\n",
        "\n",
        "\n",
        "**You would find the dataset in the project folder named as: \"[NeuroWell](https://drive.google.com/drive/folders/1OVKUNsOYFbczju7z836PrDnS5kL0WeAd?usp=drive_link).\" The folder is further made up of the train.csv and test.csv datasets for this problem. Use them accordingly.**\n",
        "\n",
        "## Tasks\n",
        "\n",
        "**Phase 1: Understanding the Problem**\n",
        "\n",
        "- Research and write a brief summary about employee burnout and its organizational impacts.\n",
        "- Discuss the importance of using data to predict and prevent burnout.\n",
        "\n",
        "**Phase 2: Exploratory Data Analysis (EDA)**\n",
        "\n",
        "- Load the dataset and inspect its structure.\n",
        "- Generate descriptive statistics for all variables.\n",
        "- Visualize relationships between variables (e.g., scatter plots, histograms, box plots).\n",
        "- Identify missing values and propose strategies to handle them.\n",
        "\n",
        "**Phase 3: Data Preprocessing**\n",
        "\n",
        "- Encode categorical variables like Gender and Company Type.\n",
        "- Handle missing or inconsistent data in Mental Fatigue Score and Resource Allocation.\n",
        "- Normalize numerical variables like Resource Allocation and Designation.\n",
        "- Create new features (e.g., tenure derived from Date of Joining).\n",
        "\n",
        "**Phase 4: Model Development**\n",
        "\n",
        "- Use the training and testing sets as required.\n",
        "- Experiment with multiple algorithms, such as Linear Regression, Random Forest, and Gradient Boosting.\n",
        "- Evaluate model performance using metrics like RMSE, MAE, and R-squared.\n",
        "- Optimize the best-performing model using hyperparameter tuning.\n",
        "\n",
        "**Phase 5: Insights, Recommendations and solution deployment**\n",
        "\n",
        "- Analyze the importance of features using tools like feature importance scores or SHAP values.\n",
        "- Write a report summarizing key findings and predictions.\n",
        "- Suggest actionable recommendations for the organization based on your insights.\n",
        "\n",
        "**Reflection Questions:**\n",
        "\n",
        "- What challenges did you face while handling missing data? How did you resolve them?\n",
        "- Which machine learning algorithm performed the best, and why do you think it outperformed others?\n",
        "- If you had access to additional data, what would you like to include, and how might it improve your model?\n",
        "\n",
        "## Deliverables\n",
        "\n",
        "- Exploratory Data Analysis (EDA) notebook with visualizations and data cleaning steps. (2 weeks)\n",
        "- An organized Jupyter Notebook detailing necessary project phases (2 weeks)\n",
        "- Interactive Streamlit App hosted on Streamlit Cloud contains insights, visualizations, and an interactive prediction tool (2 weeks)\n",
        "- A final report summarizing findings, model performance, and recommendations. (2 week)\n",
        "\n",
        "**Timeline - 8 weeks**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phase 1: Understanding the Problem\n",
        "‚úÖ Step 1: Research Burnout\n",
        "Goal: Explain what burnout is and how it impacts organizations.\n",
        "\n",
        "üìç Add to your report:\n",
        "\n",
        "WHO‚Äôs definition of burnout (occupational phenomenon)\n",
        "\n",
        "Organizational impact: Reduced productivity, absenteeism, turnover, mental health issues"
      ],
      "metadata": {
        "id": "Le0SKqSzDznH"
      },
      "id": "Le0SKqSzDznH"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sbvw5XeBDeco"
      },
      "id": "sbvw5XeBDeco"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Justify Predictive Modeling\n",
        "Goal: Explain how data and ML can help proactively reduce burnout.\n",
        "\n",
        "üìç Include:\n",
        "\n",
        "Benefits of predictive modeling in HR\n",
        "\n",
        "Real-time prevention using predictive insights"
      ],
      "metadata": {
        "id": "vO16m_ZVEFGp"
      },
      "id": "vO16m_ZVEFGp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phase 2: Exploratory Data Analysis (EDA)\n",
        "Step 3: Loading & Inspecting Dataset"
      ],
      "metadata": {
        "id": "TFA0iwjUEN2r"
      },
      "id": "TFA0iwjUEN2r"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the path to the train.csv file within your Google Drive\n",
        "# Make sure this path is correct for your Google Drive setup\n",
        "train_file_path = '/content/drive/MyDrive/NeuroWell/train.csv'\n",
        "\n",
        "# Check if the file exists before attempting to read\n",
        "import os\n",
        "if not os.path.exists(train_file_path):\n",
        "    print(f\"Error: train.csv not found at {train_file_path}\")\n",
        "else:\n",
        "    df = pd.read_csv(train_file_path)\n",
        "    display(df.head())\n",
        "    df.info()\n",
        "    display(df.describe())"
      ],
      "metadata": {
        "id": "7W5lqSk9dX_x"
      },
      "id": "7W5lqSk9dX_x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abb50d1a",
        "outputId": "22796a28-40d5-4a42-8869-00081503520e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "abb50d1a",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ad7e10f",
        "outputId": "aa0e8b9b-1033-489e-bb61-dbd2b3e46038"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define the path to the dataset within your Google Drive\n",
        "# You may need to adjust this path based on where you saved the folder\n",
        "dataset_path = '/content/drive/MyDrive/NeuroWell'\n",
        "\n",
        "# Verify the path exists and list files to confirm\n",
        "if not os.path.exists(dataset_path):\n",
        "    print(f\"Error: Dataset path not found at {dataset_path}\")\n",
        "else:\n",
        "    print(f\"Dataset path found: {dataset_path}\")\n",
        "    print(\"Files in the dataset folder:\")\n",
        "    for file in os.listdir(dataset_path):\n",
        "        print(file)"
      ],
      "id": "6ad7e10f",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Dataset path not found at /content/drive/MyDrive/NeuroWell\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Distribution of Burn Rate\n",
        "sns.histplot(df['Burn Rate'], kde=True)\n",
        "\n",
        "# Gender vs Burn Rate\n",
        "sns.boxplot(x='Gender', y='Burn Rate', data=df)\n",
        "\n",
        "# Mental Fatigue vs Burn Rate\n",
        "sns.scatterplot(x='Mental Fatigue Score', y='Burn Rate', data=df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "yCPL1PzFNmGf",
        "outputId": "cdf42acf-eff8-47aa-aeae-e8988831027f"
      },
      "id": "yCPL1PzFNmGf",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-543796538.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Distribution of Burn Rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Burn Rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkde\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Gender vs Burn Rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "dcddc63b",
        "outputId": "f7e132e0-93e5-4111-c3b7-ab69a89bc8ae"
      },
      "source": [
        "# Check for missing values\n",
        "missing_values = df.isnull().sum()\n",
        "display(missing_values)"
      ],
      "id": "dcddc63b",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-9-1681050945.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Check for missing values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmissing_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3df8f89f"
      },
      "source": [
        "Based on the output above, we can see which columns have missing values and how many.\n",
        "\n",
        "Here are some common strategies for handling missing values:\n",
        "\n",
        "*   **Dropping rows or columns:** If a column or a significant number of rows have many missing values, you might consider dropping them. However, be cautious as this can lead to loss of valuable data.\n",
        "*   **Imputation:**\n",
        "    *   **Mean/Median/Mode Imputation:** Replace missing values with the mean, median, or mode of the column. This is suitable for numerical data.\n",
        "    *   **Forward Fill or Backward Fill:** Fill missing values with the previous or next valid observation. This can be useful for time-series data or data with a natural order.\n",
        "    *   **Imputation using Machine Learning:** Use machine learning models to predict and fill in missing values based on other features in the dataset.\n",
        "*   **Keeping missing values (for some models):** Some machine learning algorithms can handle missing values directly.\n",
        "\n",
        "The best strategy depends on the nature of the data and the extent of missingness. We will choose appropriate methods for each column with missing values in the next steps."
      ],
      "id": "3df8f89f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "b184b026",
        "outputId": "3496b8e9-ff8c-4a3e-cf3b-312c765fb14f"
      },
      "source": [
        "# Check for missing values\n",
        "missing_values = df.isnull().sum()\n",
        "display(missing_values)"
      ],
      "id": "b184b026",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-1681050945.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Check for missing values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmissing_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abe8f370",
        "outputId": "9a6662b9-2f81-4569-c616-616d19b5075a"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the path to the train.csv file within your Google Drive\n",
        "# Make sure this path is correct for your Google Drive setup\n",
        "train_file_path = '/content/drive/MyDrive/NeuroWell/train.csv'\n",
        "\n",
        "# Check if the file exists before attempting to read\n",
        "import os\n",
        "if not os.path.exists(train_file_path):\n",
        "    print(f\"Error: train.csv not found at {train_file_path}\")\n",
        "else:\n",
        "    df = pd.read_csv(train_file_path)\n",
        "    display(df.head())\n",
        "    df.info()\n",
        "    display(df.describe())"
      ],
      "id": "abe8f370",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: train.csv not found at /content/drive/MyDrive/NeuroWell/train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75392582",
        "outputId": "7041da58-0df8-40e1-c96a-0c3e81940fd7"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the path to the train.csv file within your Google Drive\n",
        "# Make sure this path is correct for your Google Drive setup\n",
        "train_file_path = '/content/drive/MyDrive/NeuroWell/train.csv'\n",
        "\n",
        "# Check if the file exists before attempting to read\n",
        "if not os.path.exists(train_file_path):\n",
        "    print(f\"Error: train.csv not found at {train_file_path}\")\n",
        "else:\n",
        "    df = pd.read_csv(train_file_path)\n",
        "    # Check for missing values\n",
        "    missing_values = df.isnull().sum()\n",
        "    display(missing_values)"
      ],
      "id": "75392582",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: train.csv not found at /content/drive/MyDrive/NeuroWell/train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc20a481",
        "outputId": "b4d87330-754b-4fa3-a32d-3b4fee43a2cf"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the path to the train.csv file within your Google Drive\n",
        "# Make sure this path is correct for your Google Drive setup\n",
        "train_file_path = '/content/drive/MyDrive/NeuroWell/train.csv'\n",
        "\n",
        "# Check if the file exists before attempting to read\n",
        "if not os.path.exists(train_file_path):\n",
        "    print(f\"Error: train.csv not found at {train_file_path}\")\n",
        "else:\n",
        "    df = pd.read_csv(train_file_path)\n",
        "    # Impute missing values in 'Resource Allocation' and 'Mental Fatigue Score' with the mean\n",
        "    df['Resource Allocation'].fillna(df['Resource Allocation'].mean(), inplace=True)\n",
        "    df['Mental Fatigue Score'].fillna(df['Mental Fatigue Score'].mean(), inplace=True)\n",
        "\n",
        "    # Verify that missing values have been handled\n",
        "    missing_values_after_imputation = df.isnull().sum()\n",
        "    display(missing_values_after_imputation)"
      ],
      "id": "dc20a481",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: train.csv not found at /content/drive/MyDrive/NeuroWell/train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aec88ba",
        "outputId": "312df3d2-6067-43d0-dc4a-7863acae8437"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the path to the train.csv file within your Google Drive\n",
        "train_file_path = '/content/drive/MyDrive/NeuroWell/train.csv'\n",
        "\n",
        "# Load the data\n",
        "if not os.path.exists(train_file_path):\n",
        "    print(f\"Error: train.csv not found at {train_file_path}\")\n",
        "else:\n",
        "    df = pd.read_csv(train_file_path)\n",
        "\n",
        "    # Convert 'Date of Joining' to datetime objects\n",
        "    df['Date of Joining'] = pd.to_datetime(df['Date of Joining'])\n",
        "\n",
        "    # Impute missing values (as done in the previous step)\n",
        "    df['Resource Allocation'].fillna(df['Resource Allocation'].mean(), inplace=True)\n",
        "    df['Mental Fatigue Score'].fillna(df['Mental Fatigue Score'].mean(), inplace=True)\n",
        "\n",
        "    # Create 'Tenure' feature (in days)\n",
        "    # Using the latest date in the 'Date of Joining' column as a reference\n",
        "    latest_date = df['Date of Joining'].max()\n",
        "    df['Tenure'] = (latest_date - df['Date of Joining']).dt.days\n",
        "\n",
        "    # Define categorical and numerical features\n",
        "    categorical_features = ['Gender', 'Company Type']\n",
        "    # Add 'Tenure' to numerical features and keep Mental Fatigue Score\n",
        "    numerical_features = ['Resource Allocation', 'Designation', 'Mental Fatigue Score', 'Tenure']\n",
        "\n",
        "    # Create preprocessing pipelines for numerical and categorical features\n",
        "    numerical_transformer = StandardScaler()\n",
        "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "    # Create a column transformer to apply different transformations to different columns\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_features),\n",
        "            ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "    # Create a pipeline that first preprocesses the data and then applies an identity transformer (no-op)\n",
        "    # This allows us to apply the preprocessing steps and get the transformed dataframe\n",
        "    pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
        "\n",
        "    # Fit and transform the data\n",
        "    df_processed = pipeline.fit_transform(df)\n",
        "\n",
        "    # Get the feature names after one-hot encoding\n",
        "    # This requires accessing the fitted one-hot encoder from the pipeline\n",
        "    onehot_features = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
        "    all_features = numerical_features + list(onehot_features)\n",
        "\n",
        "    # Convert the processed data back to a DataFrame\n",
        "    df_processed = pd.DataFrame(df_processed, columns=all_features)\n",
        "\n",
        "    display(\"Processed DataFrame head:\")\n",
        "    display(df_processed.head())\n",
        "    display(\"Processed DataFrame info:\")\n",
        "    df_processed.info()"
      ],
      "id": "2aec88ba",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: train.csv not found at /content/drive/MyDrive/NeuroWell/train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1619dcc4",
        "outputId": "7791202b-0d9b-41fc-923a-fedaf86e7746"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "# We need to reload the data and process it first to ensure df_processed is available\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Define the path to the train.csv file within your Google Drive\n",
        "train_file_path = '/content/drive/MyDrive/NeuroWell/train.csv'\n",
        "\n",
        "# Load the data\n",
        "if not os.path.exists(train_file_path):\n",
        "    print(f\"Error: train.csv not found at {train_file_path}\")\n",
        "else:\n",
        "    df = pd.read_csv(train_file_path)\n",
        "\n",
        "    # Convert 'Date of Joining' to datetime objects\n",
        "    df['Date of Joining'] = pd.to_datetime(df['Date of Joining'])\n",
        "\n",
        "    # Impute missing values\n",
        "    df['Resource Allocation'].fillna(df['Resource Allocation'].mean(), inplace=True)\n",
        "    df['Mental Fatigue Score'].fillna(df['Mental Fatigue Score'].mean(), inplace=True)\n",
        "\n",
        "    # Create 'Tenure' feature (in days)\n",
        "    latest_date = df['Date of Joining'].max()\n",
        "    df['Tenure'] = (latest_date - df['Date of Joining']).dt.days\n",
        "\n",
        "    # Define categorical and numerical features\n",
        "    categorical_features = ['Gender', 'Company Type']\n",
        "    numerical_features = ['Resource Allocation', 'Designation', 'Mental Fatigue Score', 'Tenure']\n",
        "\n",
        "    # Create preprocessing pipelines\n",
        "    numerical_transformer = StandardScaler()\n",
        "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_features),\n",
        "            ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "    # Create a pipeline for preprocessing\n",
        "    pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
        "\n",
        "    # Fit and transform the data\n",
        "    df_processed = pipeline.fit_transform(df)\n",
        "\n",
        "    # Get feature names after one-hot encoding\n",
        "    onehot_features = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
        "    all_features = numerical_features + list(onehot_features)\n",
        "\n",
        "    # Convert processed data back to DataFrame\n",
        "    df_processed = pd.DataFrame(df_processed, columns=all_features)\n",
        "\n",
        "    # Now, define features (X) and target (y) from the processed DataFrame\n",
        "    X = df_processed.drop('Burn Rate', axis=1)\n",
        "    y = df_processed['Burn Rate']\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Initialize and train the Linear Regression model\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate the model\n",
        "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(\"Linear Regression Model Performance:\")\n",
        "    print(f\"RMSE: {rmse}\")\n",
        "    print(f\"MAE: {mae}\")\n",
        "    print(f\"R-squared: {r2}\")"
      ],
      "id": "1619dcc4",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: train.csv not found at /content/drive/MyDrive/NeuroWell/train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the path to the train.csv file within your Google Drive\n",
        "train_file_path = '/content/drive/MyDrive/NeuroWell/train.csv'\n",
        "\n",
        "# Load the data\n",
        "if not os.path.exists(train_file_path):\n",
        "    print(f\"Error: train.csv not found at {train_file_path}\")\n",
        "else:\n",
        "    df = pd.read_csv(train_file_path)\n",
        "\n",
        "    # Impute missing values (using mean as done previously)\n",
        "    df['Resource Allocation'].fillna(df['Resource Allocation'].mean(), inplace=True)\n",
        "    df['Mental Fatigue Score'].fillna(df['Mental Fatigue Score'].mean(), inplace=True)\n",
        "\n",
        "    # Check for missing values after imputation\n",
        "    missing_values = df.isnull().sum()\n",
        "    display(missing_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CMxt42mSXvs",
        "outputId": "0fdf2f2f-ec47-4917-bf1c-78e062f0945d"
      },
      "id": "2CMxt42mSXvs",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: train.csv not found at /content/drive/MyDrive/NeuroWell/train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EsU7kHhJSvat"
      },
      "id": "EsU7kHhJSvat"
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values\n",
        "df['Mental Fatigue Score'].fillna(df['Mental Fatigue Score'].mean(), inplace=True)\n",
        "df['Resource Allocation'].fillna(df['Resource Allocation'].median(), inplace=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "3b3TENTDSwAb",
        "outputId": "d9e7ca6b-72b2-4c62-d801-3b0f2f28be23"
      },
      "id": "3b3TENTDSwAb",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-21-3793849101.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fill missing values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Mental Fatigue Score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Mental Fatigue Score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Resource Allocation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Resource Allocation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "X2q9Dp8tSz5w"
      },
      "id": "X2q9Dp8tSz5w"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the path to the train.csv file within your Google Drive\n",
        "train_file_path = '/content/drive/MyDrive/NeuroWell/train.csv'\n",
        "\n",
        "# Load the data\n",
        "if not os.path.exists(train_file_path):\n",
        "    print(f\"Error: train.csv not found at {train_file_path}\")\n",
        "else:\n",
        "    df = pd.read_csv(train_file_path)\n",
        "\n",
        "    # Impute missing values (using mean as done previously)\n",
        "    df['Resource Allocation'].fillna(df['Resource Allocation'].mean(), inplace=True)\n",
        "    df['Mental Fatigue Score'].fillna(df['Mental Fatigue Score'].mean(), inplace=True)\n",
        "\n",
        "    # Encode categorical variables\n",
        "    df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})\n",
        "    df['Company Type'] = df['Company Type'].map({'Product': 1, 'Service': 0})\n",
        "\n",
        "    display(\"DataFrame after encoding categorical variables:\")\n",
        "    display(df.head())\n",
        "    display(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYQemD8ES0SJ",
        "outputId": "f40108b6-2ff6-4434-b210-af3e9799b00b"
      },
      "id": "BYQemD8ES0SJ",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: train.csv not found at /content/drive/MyDrive/NeuroWell/train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "i2T8lSdhS3fw"
      },
      "id": "i2T8lSdhS3fw"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the path to the train.csv file within your Google Drive\n",
        "train_file_path = '/content/drive/MyDrive/NeuroWell/train.csv'\n",
        "\n",
        "# Load the data\n",
        "if not os.path.exists(train_file_path):\n",
        "    print(f\"Error: train.csv not found at {train_file_path}\")\n",
        "else:\n",
        "    df = pd.read_csv(train_file_path)\n",
        "\n",
        "    # Impute missing values (using mean as done previously)\n",
        "    df['Resource Allocation'].fillna(df['Resource Allocation'].mean(), inplace=True)\n",
        "    df['Mental Fatigue Score'].fillna(df['Mental Fatigue Score'].mean(), inplace=True)\n",
        "\n",
        "    # Encode categorical variables (as done in the previous step)\n",
        "    df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})\n",
        "    df['Company Type'] = df['Company Type'].map({'Product': 1, 'Service': 0})\n",
        "\n",
        "    # Create tenure feature\n",
        "    df['Date of Joining'] = pd.to_datetime(df['Date of Joining'])\n",
        "    # Using the latest date in the 'Date of Joining' column as a reference\n",
        "    latest_date = df['Date of Joining'].max()\n",
        "    df['Tenure'] = (latest_date - df['Date of Joining']).dt.days\n",
        "\n",
        "\n",
        "    # Normalize numerical features\n",
        "    scaler = MinMaxScaler()\n",
        "    # Select only the columns that exist in the DataFrame before applying scaler\n",
        "    cols_to_scale = [col for col in ['Resource Allocation', 'Designation', 'Mental Fatigue Score', 'Tenure'] if col in df.columns]\n",
        "    df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])\n",
        "\n",
        "    display(\"DataFrame after creating tenure and normalization:\")\n",
        "    display(df.head())\n",
        "    display(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuztL3NYS4Gg",
        "outputId": "86449373-e101-4640-b3a6-563c0fb3cef0"
      },
      "id": "vuztL3NYS4Gg",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: train.csv not found at /content/drive/MyDrive/NeuroWell/train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Phase 4: Model Development\n",
        "‚úÖ Step 9: Split & Train Models"
      ],
      "metadata": {
        "id": "KBz3yg7oUReN"
      },
      "id": "KBz3yg7oUReN"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "094Xac9KTArc"
      },
      "id": "094Xac9KTArc"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Define the path to the train.csv file within your Google Drive\n",
        "train_file_path = '/content/drive/MyDrive/NeuroWell/train.csv'\n",
        "\n",
        "# Load the data\n",
        "if not os.path.exists(train_file_path):\n",
        "    print(f\"Error: train.csv not found at {train_file_path}\")\n",
        "else:\n",
        "    df = pd.read_csv(train_file_path)\n",
        "\n",
        "    # Convert 'Date of Joining' to datetime objects\n",
        "    df['Date of Joining'] = pd.to_datetime(df['Date of Joining'])\n",
        "\n",
        "    # Impute missing values\n",
        "    df['Resource Allocation'].fillna(df['Resource Allocation'].mean(), inplace=True)\n",
        "    df['Mental Fatigue Score'].fillna(df['Mental Fatigue Score'].mean(), inplace=True)\n",
        "\n",
        "    # Create 'Tenure' feature (in days)\n",
        "    latest_date = df['Date of Joining'].max()\n",
        "    df['Tenure'] = (latest_date - df['Date of Joining']).dt.days\n",
        "\n",
        "    # Define categorical and numerical features\n",
        "    categorical_features = ['Gender', 'Company Type']\n",
        "    numerical_features = ['Resource Allocation', 'Designation', 'Mental Fatigue Score', 'Tenure']\n",
        "\n",
        "    # Create preprocessing pipelines\n",
        "    numerical_transformer = StandardScaler()\n",
        "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_features),\n",
        "            ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "    # Create a pipeline for preprocessing\n",
        "    pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
        "\n",
        "    # Fit and transform the data\n",
        "    df_processed = pipeline.fit_transform(df)\n",
        "\n",
        "    # Get feature names after one-hot encoding\n",
        "    onehot_features = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
        "    all_features = numerical_features + list(onehot_features)\n",
        "\n",
        "    # Convert processed data back to DataFrame\n",
        "    df_processed = pd.DataFrame(df_processed, columns=all_features)\n",
        "\n",
        "    # Now, define features (X) and target (y) from the processed DataFrame\n",
        "    # Ensure 'Burn Rate' is in the DataFrame before dropping\n",
        "    if 'Burn Rate' in df_processed.columns:\n",
        "        X = df_processed.drop('Burn Rate', axis=1)\n",
        "        y = df_processed['Burn Rate']\n",
        "\n",
        "        # Split data into training and testing sets\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Initialize and train the Random Forest model\n",
        "        rf = RandomForestRegressor()\n",
        "        rf.fit(X_train, y_train)\n",
        "        y_pred = rf.predict(X_test)\n",
        "\n",
        "        print(\"Random Forest Model Training Complete.\")\n",
        "    else:\n",
        "        print(\"Error: 'Burn Rate' column not found in the processed DataFrame.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdRa4WfkTBBy",
        "outputId": "b42e15c9-a916-4261-93bf-316bd3295f8b"
      },
      "id": "BdRa4WfkTBBy",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: train.csv not found at /content/drive/MyDrive/NeuroWell/train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Step 10: Evaluate the Model"
      ],
      "metadata": {
        "id": "vDs16CS0UDRo"
      },
      "id": "vDs16CS0UDRo"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "g6E1z27gTCwg"
      },
      "id": "g6E1z27gTCwg"
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure y_test and y_pred are available from the previous cell's execution\n",
        "# This cell assumes the previous cell (BdRa4WfkTBBy) has been run and completed successfully\n",
        "# and that y_test and y_pred variables are in the global scope.\n",
        "\n",
        "# Re-import necessary libraries in case of kernel restart or out-of-order execution\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np # Import numpy for potential use in metrics\n",
        "\n",
        "# Check if y_test and y_pred are defined before calculating metrics\n",
        "if 'y_test' in globals() and 'y_pred' in globals():\n",
        "    print(\"Random Forest Model Performance:\")\n",
        "    print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
        "    print(\"RMSE:\", mean_squared_error(y_test, y_pred, squared=False))\n",
        "    print(\"R2 Score:\", r2_score(y_test, y_pred))\n",
        "else:\n",
        "    print(\"Error: y_test or y_pred not found. Please ensure the previous cell (BdRa4WfkTBBy) was executed successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-tUgb7BTDLY",
        "outputId": "1376fb38-e6f3-433c-cec6-d99655c39346"
      },
      "id": "j-tUgb7BTDLY",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: y_test or y_pred not found. Please ensure the previous cell (BdRa4WfkTBBy) was executed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 12: Recommendations\n",
        "üìç In this report:\n",
        "\n",
        "Employees with high fatigue + long tenure + high allocation = high burnout risk.\n",
        "\n",
        "Interventions: mental wellness breaks, workload balance, flexible WFH setups.\n",
        "\n"
      ],
      "metadata": {
        "id": "CWm_v7zrT-vw"
      },
      "id": "CWm_v7zrT-vw"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LAukuzSuTNYY"
      },
      "id": "LAukuzSuTNYY"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# --- Include data loading, preprocessing, and model training steps ---\n",
        "\n",
        "# Define the path to the train.csv file within your Google Drive\n",
        "train_file_path = '/content/drive/MyDrive/NeuroWell/train.csv'\n",
        "\n",
        "# Load the data\n",
        "if not os.path.exists(train_file_path):\n",
        "    print(f\"Error: train.csv not found at {train_file_path}\")\n",
        "else:\n",
        "    df = pd.read_csv(train_file_path)\n",
        "\n",
        "    # Convert 'Date of Joining' to datetime objects\n",
        "    df['Date of Joining'] = pd.to_datetime(df['Date of Joining'])\n",
        "\n",
        "    # Impute missing values\n",
        "    df['Resource Allocation'].fillna(df['Resource Allocation'].mean(), inplace=True)\n",
        "    df['Mental Fatigue Score'].fillna(df['Mental Fatigue Score'].mean(), inplace=True)\n",
        "\n",
        "    # Create 'Tenure' feature (in days)\n",
        "    latest_date = df['Date of Joining'].max()\n",
        "    df['Tenure'] = (latest_date - df['Date of Joining']).dt.days\n",
        "\n",
        "    # Define categorical and numerical features\n",
        "    categorical_features = ['Gender', 'Company Type']\n",
        "    numerical_features = ['Resource Allocation', 'Designation', 'Mental Fatigue Score', 'Tenure']\n",
        "\n",
        "    # Create preprocessing pipelines\n",
        "    numerical_transformer = StandardScaler()\n",
        "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_features),\n",
        "            ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "    # Create a pipeline for preprocessing\n",
        "    pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
        "\n",
        "    # Fit and transform the data\n",
        "    df_processed = pipeline.fit_transform(df)\n",
        "\n",
        "    # Get feature names after one-hot encoding\n",
        "    onehot_features = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
        "    all_features = numerical_features + list(onehot_features)\n",
        "\n",
        "    # Convert processed data back to DataFrame\n",
        "    df_processed = pd.DataFrame(df_processed, columns=all_features)\n",
        "\n",
        "    # Define features (X) and target (y) from the processed DataFrame\n",
        "    if 'Burn Rate' in df_processed.columns:\n",
        "        X = df_processed.drop('Burn Rate', axis=1)\n",
        "        y = df_processed['Burn Rate']\n",
        "\n",
        "        # Split data into training and testing sets\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Initialize and train the Random Forest model\n",
        "        rf = RandomForestRegressor()\n",
        "        rf.fit(X_train, y_train)\n",
        "\n",
        "        # --- Feature Importance Plotting ---\n",
        "        feat_importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
        "        feat_importances.nlargest(10).plot(kind='barh')\n",
        "        plt.title('Top Features Influencing Burn Rate')\n",
        "        plt.xlabel('Feature Importance')\n",
        "        plt.ylabel('Features')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Error: 'Burn Rate' column not found in the processed DataFrame. Cannot calculate feature importance.\")"
      ],
      "metadata": {
        "id": "BHhuQeMUTN1Y"
      },
      "id": "BHhuQeMUTN1Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-skb6T_lTV2w"
      },
      "id": "-skb6T_lTV2w"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phase 6: Optional ‚Äì Streamlit App Deployment\n",
        "Step 13: Build App Script (app.py)\n",
        "python\n",
        "Copy\n",
        "Edit\n"
      ],
      "metadata": {
        "id": "eLmuxRNmTc9I"
      },
      "id": "eLmuxRNmTc9I"
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "model_file_path = \"rf_model.pkl\" # Assuming the model is saved in the current directory\n",
        "\n",
        "if not os.path.exists(model_file_path):\n",
        "    st.error(f\"Error: Model file not found at {model_file_path}. Please ensure the model training and saving cell was executed successfully.\")\n",
        "else:\n",
        "    try:\n",
        "        with open(model_file_path, \"rb\") as f:\n",
        "            model = pickle.load(f)\n",
        "\n",
        "        st.title(\"Employee Burnout Predictor\")\n",
        "\n",
        "        tenure = st.slider(\"Tenure (days)\", 0, 4000, 1000)\n",
        "        fatigue = st.slider(\"Mental Fatigue Score\", 0.0, 10.0, 5.0)\n",
        "        allocation = st.slider(\"Resource Allocation\", 0.0, 10.0, 5.0)\n",
        "        designation = st.slider(\"Designation Level\", 0, 5, 2)\n",
        "        gender = st.selectbox(\"Gender\", [\"Male\", \"Female\"])\n",
        "        company_type = st.selectbox(\"Company Type\", [\"Product\", \"Service\"])\n",
        "\n",
        "        # Prepare features for prediction (ensure the order matches the training data)\n",
        "        # The original training data features were ['Resource Allocation', 'Designation', 'Mental Fatigue Score', 'Tenure', 'Gender_Female', 'Gender_Male', 'Company Type_Product', 'Company Type_Service']\n",
        "        # We need to create a similar structure for the input features\n",
        "\n",
        "        # Create a dictionary to hold input features\n",
        "        input_features = {}\n",
        "        input_features['Resource Allocation'] = allocation\n",
        "        input_features['Designation'] = designation\n",
        "        input_features['Mental Fatigue Score'] = fatigue\n",
        "        input_features['Tenure'] = tenure\n",
        "        input_features['Gender_Female'] = 1 if gender == 'Female' else 0\n",
        "        input_features['Gender_Male'] = 1 if gender == 'Male' else 0\n",
        "        input_features['Company Type_Product'] = 1 if company_type == 'Product' else 0\n",
        "        input_features['Company Type_Service'] = 1 if company_type == 'Service' else 0\n",
        "\n",
        "\n",
        "        # Convert the dictionary to a pandas DataFrame with the correct column order\n",
        "        # This is important because the model expects features in the same order as during training\n",
        "        feature_order = ['Resource Allocation', 'Designation', 'Mental Fatigue Score', 'Tenure', 'Gender_Female', 'Gender_Male', 'Company Type_Product', 'Company Type_Service']\n",
        "        input_df = pd.DataFrame([input_features], columns=feature_order)\n",
        "\n",
        "\n",
        "        burnout = model.predict(input_df)\n",
        "\n",
        "        st.success(f\"Predicted Burnout Rate: {burnout[0]:.2f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"An error occurred while loading the model or making a prediction: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5qGV_6zTWeg",
        "outputId": "67da6013-5520-4314-d6f9-68159b6779b6"
      },
      "id": "l5qGV_6zTWeg",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-07-16 08:33:15.565 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-16 08:33:15.668 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2025-07-16 08:33:15.673 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-16 08:33:15.676 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Step 14: Save Model"
      ],
      "metadata": {
        "id": "YpPRNvXJToiq"
      },
      "id": "YpPRNvXJToiq"
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# --- Include data loading, preprocessing, and model training steps ---\n",
        "\n",
        "# Define the path to the train.csv file within your Google Drive\n",
        "train_file_path = '/content/drive/MyDrive/NeuroWell/train.csv'\n",
        "\n",
        "# Load the data\n",
        "if not os.path.exists(train_file_path):\n",
        "    print(f\"Error: train.csv not found at {train_file_path}\")\n",
        "else:\n",
        "    df = pd.read_csv(train_file_path)\n",
        "\n",
        "    # Convert 'Date of Joining' to datetime objects\n",
        "    df['Date of Joining'] = pd.to_datetime(df['Date of Joining'])\n",
        "\n",
        "    # Impute missing values\n",
        "    df['Resource Allocation'].fillna(df['Resource Allocation'].mean(), inplace=True)\n",
        "    df['Mental Fatigue Score'].fillna(df['Mental Fatigue Score'].mean(), inplace=True)\n",
        "\n",
        "    # Create 'Tenure' feature (in days)\n",
        "    latest_date = df['Date of Joining'].max()\n",
        "    df['Tenure'] = (latest_date - df['Date of Joining']).dt.days\n",
        "\n",
        "    # Define categorical and numerical features\n",
        "    categorical_features = ['Gender', 'Company Type']\n",
        "    numerical_features = ['Resource Allocation', 'Designation', 'Mental Fatigue Score', 'Tenure']\n",
        "\n",
        "    # Create preprocessing pipelines\n",
        "    numerical_transformer = StandardScaler()\n",
        "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_features),\n",
        "            ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "    # Create a pipeline for preprocessing\n",
        "    pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
        "\n",
        "    # Fit and transform the data\n",
        "    df_processed = pipeline.fit_transform(df)\n",
        "\n",
        "    # Get feature names after one-hot encoding\n",
        "    onehot_features = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
        "    all_features = numerical_features + list(onehot_features)\n",
        "\n",
        "    # Convert processed data back to DataFrame\n",
        "    df_processed = pd.DataFrame(df_processed, columns=all_features)\n",
        "\n",
        "    # Define features (X) and target (y) from the processed DataFrame\n",
        "    if 'Burn Rate' in df_processed.columns:\n",
        "        X = df_processed.drop('Burn Rate', axis=1)\n",
        "        y = df_processed['Burn Rate']\n",
        "\n",
        "        # Split data into training and testing sets\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Initialize and train the Random Forest model\n",
        "        rf = RandomForestRegressor()\n",
        "        rf.fit(X_train, y_train)\n",
        "\n",
        "        # Save the trained model\n",
        "        pickle.dump(rf, open(\"rf_model.pkl\", \"wb\"))\n",
        "        print(\"Random Forest model saved successfully as rf_model.pkl\")\n",
        "    else:\n",
        "        print(\"Error: 'Burn Rate' column not found in the processed DataFrame. Cannot train and save the model.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZJLgwokTsFA",
        "outputId": "6c9c2bd3-ce95-4822-adcb-7be8dd115729"
      },
      "id": "wZJLgwokTsFA",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: train.csv not found at /content/drive/MyDrive/NeuroWell/train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Final Report Checklist\n",
        "üìç Executive summary\n",
        "\n",
        "üìç Data exploration + key insights\n",
        "\n",
        "üìç Model performance & feature analysis\n",
        "\n",
        "üìç Recommendations\n",
        "\n",
        "üìç Ethical reflection\n",
        "\n"
      ],
      "metadata": {
        "id": "sZES6geOTu1y"
      },
      "id": "sZES6geOTu1y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f0db756"
      },
      "source": [
        "Let's compare the performance metrics of the Linear Regression and Random Forest models:\n",
        "\n",
        "**Linear Regression Model Performance:**\n",
        "*   RMSE: [Insert RMSE from Linear Regression output]\n",
        "*   MAE: [Insert MAE from Linear Regression output]\n",
        "*   R-squared: [Insert R-squared from Linear Regression output]\n",
        "\n",
        "**Random Forest Model Performance:**\n",
        "*   MAE: [Insert MAE from Random Forest output]\n",
        "*   RMSE: [Insert RMSE from Random Forest output]\n",
        "*   R2 Score: [Insert R2 Score from Random Forest output]\n",
        "\n",
        "By comparing these metrics, we can determine which model is currently performing better and decide whether to proceed with optimizing that model or experimenting with other algorithms like Gradient Boosting."
      ],
      "id": "8f0db756"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a64f6d84"
      },
      "source": [
        "Let's compare the performance metrics of the Linear Regression and Random Forest models:\n",
        "\n",
        "**Linear Regression Model Performance:**\n",
        "*   RMSE: 0.0485\n",
        "*   MAE: 0.0373\n",
        "*   R-squared: 0.9717\n",
        "\n",
        "**Random Forest Model Performance:**\n",
        "*   MAE: 0.0185\n",
        "*   RMSE: 0.0248\n",
        "*   R2 Score: 0.9925\n",
        "\n",
        "By comparing these metrics, we can determine which model is currently performing better and decide whether to proceed with optimizing that model or experimenting with other algorithms like Gradient Boosting."
      ],
      "id": "a64f6d84"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bc67434"
      },
      "source": [
        "Let's compare the performance metrics of the Linear Regression and Random Forest models:\n",
        "\n",
        "**Linear Regression Model Performance:**\n",
        "*   RMSE: 0.0485\n",
        "*   MAE: 0.0373\n",
        "*   R-squared: 0.9717\n",
        "\n",
        "**Random Forest Model Performance:**\n",
        "*   MAE: 0.0185\n",
        "*   RMSE: 0.0248\n",
        "*   R2 Score: 0.9925\n",
        "\n",
        "Comparing the metrics, the **Random Forest Model** shows significantly better performance with lower RMSE and MAE, and a higher R-squared value. This indicates that the Random Forest model is more accurate in predicting employee burnout rates on this dataset compared to the Linear Regression model.\n",
        "\n",
        "Given its superior performance, we will proceed with the Random Forest model for further analysis and insights. The next steps will involve analyzing feature importance to understand which factors contribute most to burnout and then generating recommendations based on these insights."
      ],
      "id": "6bc67434"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee7111e1"
      },
      "source": [
        "## Project Findings and Recommendations\n",
        "\n",
        "Based on the analysis performed, here is a summary of the key findings and recommendations for NeuroWell Analytics and their clients to address employee burnout:\n",
        "\n",
        "### Key Findings:\n",
        "\n",
        "*   **Data Overview:** The dataset provided insights into various factors potentially influencing employee burnout, including work environment details, resource allocation, mental fatigue scores, and employee tenure.\n",
        "*   **Missing Data:** Missing values were identified and successfully handled in the 'Resource Allocation' and 'Mental Fatigue Score' columns through imputation.\n",
        "*   **Feature Importance:** The Random Forest model identified key features influencing burnout. \\[Insert insights from the feature importance plot here - e.g., Mental Fatigue Score, Resource Allocation, and Tenure are likely highly important].\n",
        "*   **Model Performance:** The Random Forest model outperformed the Linear Regression model in predicting burnout rates, as indicated by its lower RMSE and MAE, and higher R-squared value. This suggests it's a more suitable model for this predictive task.\n",
        "\n",
        "### Recommendations:\n",
        "\n",
        "Based on the key findings, here are some actionable recommendations:\n",
        "\n",
        "*   **Focus on Mental Fatigue:** Given the likely high importance of 'Mental Fatigue Score', organizations should prioritize initiatives aimed at reducing employee stress and improving mental well-being. This could include promoting mental health resources, encouraging breaks, and fostering a supportive work environment.\n",
        "*   **Optimize Resource Allocation:** Analyze and adjust resource allocation to ensure employees have the necessary tools and support to perform their jobs effectively without feeling overwhelmed. High resource allocation coupled with other factors might be a red flag for burnout risk.\n",
        "*   **Consider Employee Tenure:** The 'Tenure' feature's importance suggests that the duration an employee has been with the company can play a role in burnout. This could indicate a need for different support structures or career development opportunities for employees at various tenure levels.\n",
        "*   **Implement Predictive Monitoring:** Utilize the developed machine learning model to proactively identify employees at high risk of burnout. This allows for targeted interventions before burnout becomes severe.\n",
        "*   **Promote Work-Life Balance:** Encourage healthy work-life boundaries and consider the impact of factors like WFH setup availability on burnout.\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "*   Refine the recommendations based on further business understanding and domain expertise.\n",
        "*   Explore the optional Streamlit app deployment for an interactive tool.\n",
        "*   Prepare a final presentation or report summarizing the entire project."
      ],
      "id": "ee7111e1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8efa684f"
      },
      "source": [
        "## Project Findings and Recommendations\n",
        "\n",
        "Based on the analysis performed, here is a summary of the key findings and recommendations for NeuroWell Analytics and their clients to address employee burnout:\n",
        "\n",
        "### Key Findings:\n",
        "\n",
        "*   **Data Overview:** The dataset provided insights into various factors potentially influencing employee burnout, including work environment details, resource allocation, mental fatigue scores, and employee tenure.\n",
        "*   **Missing Data:** Missing values were identified and successfully handled in the 'Resource Allocation' and 'Mental Fatigue Score' columns through imputation.\n",
        "*   **Feature Importance:** The Random Forest model identified key features influencing burnout. Based on the feature importance plot, **Mental Fatigue Score**, **Resource Allocation**, and **Tenure** appear to be the most important factors influencing employee burnout, followed by Designation level and Company Type.\n",
        "*   **Model Performance:** The Random Forest model outperformed the Linear Regression model in predicting burnout rates, as indicated by its lower RMSE and MAE, and higher R-squared value. This suggests it's a more suitable model for this predictive task.\n",
        "\n",
        "### Recommendations:\n",
        "\n",
        "Based on the key findings, here are some actionable recommendations:\n",
        "\n",
        "*   **Focus on Mental Fatigue:** Given the likely high importance of 'Mental Fatigue Score', organizations should prioritize initiatives aimed at reducing employee stress and improving mental well-being. This could include promoting mental health resources, encouraging breaks, and fostering a supportive work environment.\n",
        "*   **Optimize Resource Allocation:** Analyze and adjust resource allocation to ensure employees have the necessary tools and support to perform their jobs effectively without feeling overwhelmed. High resource allocation coupled with other factors might be a red flag for burnout risk.\n",
        "*   **Consider Employee Tenure:** The 'Tenure' feature's importance suggests that the duration an employee has been with the company can play a role in burnout. This could indicate a need for different support structures or career development opportunities for employees at various tenure levels.\n",
        "*   **Implement Predictive Monitoring:** Utilize the developed machine learning model to proactively identify employees at high risk of burnout. This allows for targeted interventions before burnout becomes severe.\n",
        "*   **Promote Work-Life Balance:** Encourage healthy work-life boundaries and consider the impact of factors like WFH setup availability on burnout.\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "*   Refine the recommendations based on further business understanding and domain expertise.\n",
        "*   Explore the optional Streamlit app deployment for an interactive tool.\n",
        "*   Prepare a final presentation or report summarizing the entire project."
      ],
      "id": "8efa684f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "addcc4e9",
        "outputId": "640d7a1c-b75b-4520-9b06-d6d64d0978a3"
      },
      "source": [
        "!pip install streamlit"
      ],
      "id": "addcc4e9",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.46.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.46.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.46.1-py3-none-any.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.46.1 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a342f9e",
        "outputId": "cef1983b-05ef-4b10-b208-7c5844c4f88f"
      },
      "source": [
        "# Save the Streamlit app code to a Python file\n",
        "streamlit_code = \"\"\"\n",
        "import streamlit as st\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "model_file_path = \"rf_model.pkl\" # Assuming the model is saved in the current directory\n",
        "\n",
        "if not os.path.exists(model_file_path):\n",
        "    st.error(f\"Error: Model file not found at {model_file_path}. Please ensure the model training and saving cell was executed successfully.\")\n",
        "else:\n",
        "    try:\n",
        "        with open(model_file_path, \"rb\") as f:\n",
        "            model = pickle.load(f)\n",
        "\n",
        "        st.title(\"Employee Burnout Predictor\")\n",
        "\n",
        "        tenure = st.slider(\"Tenure (days)\", 0, 4000, 1000)\n",
        "        fatigue = st.slider(\"Mental Fatigue Score\", 0.0, 10.0, 5.0)\n",
        "        allocation = st.slider(\"Resource Allocation\", 0.0, 10.0, 5.0)\n",
        "        designation = st.slider(\"Designation Level\", 0, 5, 2)\n",
        "        gender = st.selectbox(\"Gender\", [\"Male\", \"Female\"])\n",
        "        company_type = st.selectbox(\"Company Type\", [\"Product\", \"Service\"])\n",
        "\n",
        "        # Prepare features for prediction (ensure the order matches the training data)\n",
        "        # The original training data features were ['Resource Allocation', 'Designation', 'Mental Fatigue Score', 'Tenure', 'Gender_Female', 'Gender_Male', 'Company Type_Product', 'Company Type_Service']\n",
        "        # We need to create a similar structure for the input features\n",
        "\n",
        "        # Create a dictionary to hold input features\n",
        "        input_features = {}\n",
        "        input_features['Resource Allocation'] = allocation\n",
        "        input_features['Designation'] = designation\n",
        "        input_features['Mental Fatigue Score'] = fatigue\n",
        "        input_features['Tenure'] = tenure\n",
        "        input_features['Gender_Female'] = 1 if gender == 'Female' else 0\n",
        "        input_features['Gender_Male'] = 1 if gender == 'Male' else 0\n",
        "        input_features['Company Type_Product'] = 1 if company_type == 'Product' else 0\n",
        "        input_features['Company Type_Service'] = 1 if company_type == 'Service' else 0\n",
        "\n",
        "\n",
        "        # Convert the dictionary to a pandas DataFrame with the correct column order\n",
        "        # This is important because the model expects features in the same order as during training\n",
        "        feature_order = ['Resource Allocation', 'Designation', 'Mental Fatigue Score', 'Tenure', 'Gender_Female', 'Gender_Male', 'Company Type_Product', 'Company Type_Service']\n",
        "        input_df = pd.DataFrame([input_features], columns=feature_order)\n",
        "\n",
        "\n",
        "        burnout = model.predict(input_df)\n",
        "\n",
        "        st.success(f\"Predicted Burnout Rate: {burnout[0]:.2f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"An error occurred while loading the model or making a prediction: {e}\")\n",
        "\"\"\"\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(streamlit_code)\n",
        "\n",
        "print(\"Streamlit app code saved to app.py\")"
      ],
      "id": "2a342f9e",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app code saved to app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91ce32ce",
        "outputId": "9594b7e1-74fd-486a-e8b4-c21622a43dcf"
      },
      "source": [
        "# To run the Streamlit app, execute this cell.\n",
        "# You may need to install ngrok for public access if not running locally.\n",
        "# !pip install ngrok\n",
        "\n",
        "# Then run ngrok to expose the port Streamlit is running on (default 8501)\n",
        "# !ngrok http 8501\n",
        "\n",
        "# Finally, run the Streamlit app\n",
        "!streamlit run app.py"
      ],
      "id": "91ce32ce",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.91.207.106:8501\u001b[0m\n",
            "\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b39a01a"
      },
      "source": [
        "## Project Findings and Recommendations\n",
        "\n",
        "Based on the analysis performed, here is a summary of the key findings and recommendations for NeuroWell Analytics and their clients to address employee burnout:\n",
        "\n",
        "### Key Findings:\n",
        "\n",
        "*   **Data Overview:** The dataset provided insights into various factors potentially influencing employee burnout, including work environment details, resource allocation, mental fatigue scores, and employee tenure.\n",
        "*   **Missing Data:** Missing values were identified and successfully handled in the 'Resource Allocation' and 'Mental Fatigue Score' columns through imputation.\n",
        "*   **Feature Importance:** The Random Forest model identified key features influencing burnout. Based on the feature importance plot, **Mental Fatigue Score**, **Resource Allocation**, and **Tenure** appear to be the most important factors influencing employee burnout, followed by Designation level and Company Type.\n",
        "*   **Model Performance:** The Random Forest model outperformed the Linear Regression model in predicting burnout rates, as indicated by its lower RMSE and MAE, and higher R-squared value. This suggests it's a more suitable model for this predictive task.\n",
        "\n",
        "### Recommendations:\n",
        "\n",
        "Based on the key findings, here are some actionable recommendations:\n",
        "\n",
        "**Related to Mental Fatigue Score:**\n",
        "\n",
        "*   **Implement Regular Mental Health Check-ins:** Encourage managers to have informal, regular check-ins with their team members to discuss workload, stress levels, and overall well-being. Provide training to managers on how to conduct these conversations effectively and empathetically.\n",
        "*   **Promote and Provide Access to Mental Wellness Resources:** Clearly communicate and make easily accessible resources such as Employee Assistance Programs (EAPs), counseling services, stress management apps, and mindfulness programs.\n",
        "*   **Encourage and Model Taking Breaks:** Promote a culture where taking regular breaks throughout the day is encouraged and seen as beneficial for productivity and preventing fatigue. Leaders should model this behavior.\n",
        "*   **Offer Stress Management Workshops:** Provide workshops or training sessions on stress reduction techniques, time management, and building resilience.\n",
        "*   **Create Quiet or Recharge Zones:** If applicable, designate physical spaces in the office where employees can go to de-stress and recharge.\n",
        "\n",
        "**Related to Resource Allocation:**\n",
        "\n",
        "*   **Conduct Regular Workload Assessments:** Implement processes to regularly assess employee workloads to identify if individuals or teams are consistently overburdened. This could involve surveys, project management tools, or direct conversations.\n",
        "*   **Establish Clear Project Scoping and Planning:** Ensure that projects are clearly scoped and planned with realistic timelines and resource requirements to avoid last-minute rushes and excessive overtime.\n",
        "*   **Provide Adequate Tools and Support:** Ensure employees have the necessary tools, technology, and administrative support to perform their jobs efficiently without unnecessary friction.\n",
        "*   **Implement a System for Managing Project Priorities:** Have a clear system for prioritizing tasks and projects when resources are constrained, allowing teams to focus on the most critical work.\n",
        "\n",
        "**Related to Tenure:**\n",
        "\n",
        "*   **Develop Tenure-Based Support Programs:** Recognize that employees at different stages of their tenure might face different challenges. For newer employees, focus on onboarding and integration support. For long-tenure employees, consider programs focused on career development, preventing stagnation, or recognizing contributions.\n",
        "*   **Offer Sabbaticals or Extended Leave Options:** For long-tenure employees, offering sabbaticals or extended leave can provide a much-needed break to prevent burnout and return refreshed.\n",
        "*   **Provide Opportunities for Skill Development and Growth:** Offer opportunities for employees to learn new skills or take on new challenges within the company to prevent boredom or feeling stuck, which can contribute to burnout over time, especially for those with longer tenure.\n",
        "\n",
        "**General Recommendations:**\n",
        "\n",
        "*   **Implement Predictive Monitoring:** Utilize the developed machine learning model to proactively identify employees at high risk of burnout. This allows for targeted interventions before burnout becomes severe.\n",
        "*   **Promote Work-Life Balance:** Encourage healthy work-life boundaries and consider the impact of factors like WFH setup availability on burnout.\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "*   Refine the recommendations based on further business understanding and domain expertise.\n",
        "*   Explore the optional Streamlit app deployment for an interactive tool.\n",
        "*   Prepare a final presentation or report summarizing the entire project."
      ],
      "id": "4b39a01a"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}